{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "unify_alleles = lambda x: re.sub('[*|:|-]', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLA_OF_INTEREST = \"HLAA0201\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default AAE data\n",
    "### just split to two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"curated.csv.gz\")\n",
    "df1[\"mhc\"] = df1[\"mhc\"].map(unify_alleles)\n",
    "\n",
    "df2 = pd.read_csv(\"jci.csv.gz\")\n",
    "df2[\"mhc\"] = df2[\"mhc\"].map(unify_alleles)\n",
    "\n",
    "df3 = pd.concat([df1, df2])\n",
    "df3 = df3.loc[df3[\"mhc\"] == HLA_OF_INTEREST, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"len\"] = df3[\"sequence\"].map(len)\n",
    "df3 = df3.loc[(df3[\"len\"] >= 8) & (df3[\"len\"] <= 11), :]\n",
    "df3.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14280, 3571)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(df3))\n",
    "train_size = int(len(perm) * 0.8)\n",
    "train_size, len(perm) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 10312, 10: 3243, 11: 501, 8: 224})\n",
      "Counter({9: 2557, 10: 823, 11: 137, 8: 54})\n"
     ]
    }
   ],
   "source": [
    "train_ind = perm[:train_size]\n",
    "test_ind = perm[train_size:]\n",
    "print(Counter(df3[\"len\"][train_ind]))\n",
    "print(Counter(df3[\"len\"][test_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[train_ind, :].to_csv(\"aae_train.csv.gz\", compression=\"gzip\")\n",
    "df3.loc[test_ind, :].to_csv(\"aae_test.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality AAE data\n",
    "### Find quality data and get the Abelin data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mhc_data(names, qualitative=False, hla_of_interest=None):\n",
    "    if type(names) is not list:\n",
    "        names = [names]\n",
    "\n",
    "    res = []\n",
    "    for df_name in names:\n",
    "        df = pd.read_csv(df_name)\n",
    "        \n",
    "        df[\"mhc\"] = df[\"mhc\"].map(unify_alleles)\n",
    "        df = df.loc[df[\"mhc\"] == hla_of_interest, :]\n",
    "        \n",
    "        df[\"len\"] = df[\"sequence\"].map(len)\n",
    "        df = df.loc[(df[\"len\"] >= 8) & (df[\"len\"] <= 11), :]\n",
    "\n",
    "        if qualitative:\n",
    "            df[\"binder\"] = df[\"meas\"]\n",
    "        else:\n",
    "            df[\"meas\"].values[df[\"meas\"] > 50000] = 50000\n",
    "            df[\"binder\"] = np.where(df[\"meas\"].values <= 500, 1, 0)\n",
    "            \n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        res.append(df)\n",
    "        print(df_name, \"--\", len(df), \"rows\")\n",
    "    \n",
    "    return res if len(res) > 1 else res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_mhc_data(datasets):\n",
    "\n",
    "    def _get_confidence(x):\n",
    "        x_sum = x[\"binder\"].sum()\n",
    "        if_all_zero = x_sum == 0\n",
    "        if_all_ones = x_sum == len(x)\n",
    "        if if_all_zero or if_all_ones:\n",
    "            return len(x)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    if type(datasets) is not list:\n",
    "        datasets = [datasets]\n",
    "    print(\"Pre-merge:\")\n",
    "    for df in datasets:\n",
    "        print(\" --\", len(df), \"rows\")\n",
    "\n",
    "    pd1 = pd.concat(datasets)\n",
    "    print(\"First merge:\", len(pd1), \"rows\")\n",
    "\n",
    "    tmp = pd1.groupby([\"mhc\", \"sequence\"]).apply(_get_confidence)\n",
    "    tmp2 = tmp.reset_index()\n",
    "    tmp2.columns = [\"mhc\", \"sequence\", \"confidence\"]\n",
    "\n",
    "    pd1_new = pd1.merge(tmp2).sort_values(by=[\"sequence\"])\n",
    "    print(\"Confidence stats:\")\n",
    "    counter = Counter(pd1_new[\"confidence\"].apply(str) + \"_\" + pd1_new[\"binder\"].apply(str))\n",
    "    for conf_val in range(0, max(map(lambda x: int(x.split(\"_\")[0]), counter.keys()))):\n",
    "        val_str = str(conf_val)\n",
    "        print(\" --\", val_str + \":\", counter.get(val_str + \"_0\", 0), counter.get(val_str + \"_1\", 0))\n",
    "    \n",
    "    return pd1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(train_df, test_df):\n",
    "    # Drop all intersected peptides from train_df\n",
    "    df_merged = train_df.merge(test_df.drop_duplicates(keep=False, subset=[\"mhc\", \"sequence\"]), \n",
    "                               on=[\"mhc\", \"sequence\"], how=\"left\", indicator=True)\n",
    "    print(\"before drop:\", len(df_merged))\n",
    "    df_merged = df_merged.loc[df_merged[\"_merge\"] == \"left_only\", :]\n",
    "    print(\"after drop:\", len(df_merged))\n",
    "\n",
    "    df_merged = df_merged.reset_index(drop=True)\n",
    "    df_merged[\"meas\"] = df_merged[\"meas_x\"]\n",
    "    return df_merged.drop([\"meas_x\", \"meas_y\", \"_merge\", \"binder_x\", \"len_x\", \"len_y\", \"binder_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curated.csv.gz -- 15605 rows\n",
      "mhc_data.csv.gz -- 34821 rows\n",
      "Pre-merge:\n",
      " -- 15605 rows\n",
      " -- 34821 rows\n",
      "First merge: 50426 rows\n",
      "Confidence stats:\n",
      " -- 0: 2527 2427\n",
      " -- 1: 1164 13154\n",
      " -- 2: 10652 7526\n",
      " -- 3: 7461 4128\n",
      " -- 4: 48 696\n",
      " -- 5: 70 345\n",
      " -- 6: 12 126\n",
      " -- 7: 0 42\n",
      "abelin.csv.gz -- 133818 rows\n",
      "before drop: 50426\n",
      "after drop: 47914\n"
     ]
    }
   ],
   "source": [
    "datasets = load_mhc_data([\"curated.csv.gz\", \"mhc_data.csv.gz\"], False, HLA_OF_INTEREST)\n",
    "final_df = merge_mhc_data(datasets)\n",
    "abelin_df = load_mhc_data(\"abelin.csv.gz\", True, HLA_OF_INTEREST)\n",
    "abelin_df.loc[abelin_df[\"binder\"] == 1, \"meas\"] = 200\n",
    "abelin_df.loc[abelin_df[\"binder\"] == 0, \"meas\"] = 20000\n",
    "final_df = remove_duplicates(final_df, abelin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After threshold:\n",
      " -- 31154 high-quality pts.\n",
      " -- 16760 low-quality pts.\n"
     ]
    }
   ],
   "source": [
    "confidence_threshold = 2\n",
    "counter = Counter(final_df[\"confidence\"] >= confidence_threshold)\n",
    "print(\"After threshold:\\n\", \n",
    "      \"--\", counter[True], \"high-quality pts.\\n\",\n",
    "      \"--\", counter[False], \"low-quality pts.\")\n",
    "\n",
    "final_df.loc[final_df[\"confidence\"] >= confidence_threshold, :].to_csv(\"aae_train_high.csv.gz\", compression=\"gzip\")\n",
    "final_df.loc[final_df[\"confidence\"] < confidence_threshold, :].to_csv(\"aae_train_low.csv.gz\", compression=\"gzip\")\n",
    "abelin_df.to_csv(\"aae_test_v2.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({200: 2413, 20000: 131405})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(abelin_df[\"meas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
